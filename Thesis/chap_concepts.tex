\chapter{Concepts}
\label{chap:concepts}

\section{User Interface}

\begin{figure}
	\caption{\label{fig:firstmockup}A first overview of the screen space distribution}
	\includegraphics[width=0.9\textwidth]{assets/chpt_concepts/main_touch_interface.png}
\end{figure}

\subsection{Desired position of the Android Tablet}
The application (and thus the screens within it) will be designed for the tablet to be placed in front of the operating person on a table. The person should have clear sight on the controlled robot. It seems sensible to place the tablet on the table in front of the robot while looking at it. Most interactions with the application will be performed by touch gestures using the right hand. For better usability a housing or case can be used to position the tablet at a slight angle to the table. 
% TODO FOTO VOM TABLET AUF TISCH

Interaction with the application is done using the commonly known touch gestures like
\begin{itemize}
	\item Touch (short press on the screen)
	\item Long press (finger remains on a control for a longer period of time)
	\item 1-Finger-Movement
	\item 2-Fingered gestures (\textit{Pinch-Zoom}, Rotation)
	\item 3-Fingered gestures (Rotation, Movement)
\end{itemize}

\subsection{General Screen Layout}

As a screen of a diagonal size of 10 inches (25.4cm) is very limited compared to the size of the robot's workspace good considerations have to made according to a well-designed user interface. Since we are mainly operating the robot with touch gestures, significant parts of the screen should be blank, as only few information can be displayed while the user poses his hands above or on the screen. Figure \ref{fig:firstmockup} gives a first overview of how the portions of the screen shall be distributed. The biggest part of the screen is reserved for touch interactions by the user. Since multiple approaches to control the robot shall be implemented, the method shall be selected and switched using a tabbed layout with the tabs on the top, as they then use the least space of the screen.

\subsubsection{Interlock Button}
On all screens where the robot can be remotely operated, a security interlock button shall be displayed. For the actions on the screen to have effect on the robot (i.e. to be sent to the controller) the button shall remain pressed. This implements the functionality of a dead-man-switch, stopping all robot action once released. Although this is only a software measure it should be a good solution against unwanted movements of the robot as the button can easily be released when pressed with a single finger of the left hand. \textbf{Of course, this software measure does not replace hardware safety measures like emergency switches, but only supports them.}

\subsection{Grasp Synergy Screens}

\begin{wrapfigure}[12]{l}{0.6\textwidth}
	\caption{\label{fig:screen:synergy}Synergy control screen}
	\includegraphics[width=0.6\textwidth]{assets/chpt_concepts/grasp_synergy_page}
\end{wrapfigure}

As the control of grasp synergies allows multiple different types of synergies to be selected, a drop-down selection of the synergy shall be displayed to the left side of the screen. This keeps the right side of the screen clear for better operability by right-hand users. Additionally, a cross of lines shall be displayed on the screen so the user knows where the middle of the touch interaction area is. As described later, this is particularly important in the approach with absolute synergy control (See Chapter \ref{sec:synergies:absolute}). To give the user some information about the state of a synergy, the values of significant amplitudes applied to the synergy shall also be displayed on the left hand side of the screen. A button to set the hand into the synergy's idle state (i.e. all significant amplitudes set to $0$) is also sensible to be implemented. A sample of how this screen could look like is demonstrated in Figure \ref{fig:screen:synergy}.

\subsection{Direct Fingertip Mapping Screen}

As there are no additional controls required to control the direct fingertip mapping, the control screen looks mostly like the general touch interaction screen seen in Figure \ref{fig:firstmockup}.

\subsection{Single Axis/Joint Control}

\begin{wrapfigure}[11]{r}{0.4\textwidth}
	\caption{\label{fig:axiscontrol}Axis control widget with different status indicators}
	\includegraphics[width=0.4\textwidth]{assets/chpt_concepts/AxisControlGreen}
	\includegraphics[width=0.4\textwidth]{assets/chpt_concepts/AxisControlYellow}
	\includegraphics[width=0.4\textwidth]{assets/chpt_concepts/AxisControlRed}
\end{wrapfigure}

An interface shall be implemented to give users the ability to control each joint of the robot individually. This is sensible for a variety of reasons. Firstly, it might sometimes be required to move the robot out of a specific state by moving just one axis and not by applying multiple changes at once. Possible scenarios for this use case could be a state where the robot could harm users or the environment if uncontrolled or unpredictable movements occur. An interface to control joints individually is also very practical for testing purposes, for example if one part of the robot is suspected to be broken.

Within this interface, the currently measured joint angle shall be displayed for each joint, next to the currently set target value (printed in bold). This gives the user a good insight of what state the robot should be in according to the program and what state it is actually in. This shall be supported by a coloured indicator, giving a quick visual feedback on the difference of the target and actual joint angles $\Delta \alpha = |\alpha_{actual} - \alpha_{target}|$. The colours of the visual feedback shall be:

\begin{itemize}
	\item Green for $\Delta\alpha \leq 0.5^\circ$
	\item Yellow for $0.5^\circ < \Delta\alpha \leq 2.5^\circ$
	\item Red for $\Delta\alpha > 2.5^\circ$ 
\end{itemize}

The buttons to move the axis shall be displayed right and left of the angle displays. The visual feedback shall be shown as the background of the angle values. All these requirements put together, an axis control widget for a single joint or axis could look like depicted in Figure \ref{fig:axiscontrol}. To each control widget, a heading will be added to unambiguously denote which axis or joint will be controlled when using the corresponding buttons.

Multiple of these widgets shall be added to the axis control screen, one for each controllable joint or axis. This will result in a screen containing 29 of these (22 for the hand, 7 for the robot arm). To get an idea of how the control screen for individual joint control will look like, refer to Figure \ref{fig:axiscontrol:screen}\footnote{Please note, however, that the number of hinted joint control widgets does not resemble the actual number of controllable joints for each part of the robot.}. 

\begin{figure}
	\caption{\label{fig:axiscontrol:screen}Axis control screen draft}
	\frame{\includegraphics[width=0.8\textwidth]{assets/chpt_concepts/AxisControlPage}}
\end{figure}

\section{Using the BioIK Service}
\label{sec:robotarm:ctrl}

The BioIK serivce (see Section \ref{sec:bioik}) is used according to the description in Section \ref{sec:using_services}. The important messages types are:
\begin{itemize}
	\item \textbf{GetIK} The combined Request/Response service message type
	\item \textbf{GetIKRequest} The message type containing the request to the service
	\item \textbf{GetIKResponse} The message type containing the response of the service
	\item \textbf{IKRequest} The actual request data
	\item \textbf{IKResponse} The actual response data
\end{itemize}

In the process of getting joint data from the BioIK service, the message types \textit{IKRequest} and \textit{IKResponse} are most important, as they contain the data needed on both sides. As the used message types contain a large number of members, only those significant for this thesis will be discussed within this section.

The \textit{IKRequest} message contains information about the current robot state and about the desired robot pose (see Table \ref{tab:msg:ikrequest}). The so-called \textit{goals} used here are \textit{PositionGoal}, \textit{OrientationGoal} and \textit{PoseGoal}, which is basically a combination of the first two. All goals contain a link name, which describes the end-effector that shall be brought to the given position or orientation. A \textit{PoseGoal} contains a \textit{Point} message type, which is a vector in 3D space. The \textit{OrientationGoal} consists of a \textit{Quaternion} which encodes the desired orientation of the end-effector in space. Lastly, the \textit{PoseGoal} contains both, a \textit{Point} and a \textit{Quaternion}, defining a distinct position and orientation in space. When provided with the current state of the robot, the IK algorithm begins looking for solutions at this state, possibly speeding up the overall solving process. Different goals for multiple end-effectors (called \textit{links}) can be passed to the BioIK service which will then try to find a solution fulfilling all of the given goals.

When the BioIK service has finished it returns a \textit{IKResponse} (Table \ref{tab:msg:ikresponse}) to the caller. Within the response, a status (or error) code is given, indicating success or failure of the algorithm. If the status code indicates 0 (meaning success), the \textit{RobotState} field of the message contains joint angles for all joints of the robot, representing a state in which the goals passed to the service in the first place are reached. All angles for the joints (in \textit{IKRequest} as well as \textit{IKResponse}) are given in radians.

\begin{table}
	\caption{Important contents of the IKRequest message type\label{tab:msg:ikrequest}}
	\begin{tabularx}{\linewidth}{|l|X|}
		\hline
		\textbf{Field} & \textbf{Description} \\
		\hline
		string group\_name & The MoveGroup name of the robot. Fixed to \textit{lwr\_with\_c5hand} here \\
		\hline
		bool approximate & If true, an approximate solution is returned when no exact solution could be found by the IK solver \\
		\hline
		duration timeout & The timeout after which the solver stops looking for solutions. If no solution was found by then, an approximate solution is returned if approximate is true, otherwise no solution is returned. Fixed to one second here. \\
		\hline
		int32 attempts & Number of attempts the solver shall take. Fixed to 1 here. \\
		\hline
		string[] fixed\_joints & Names of the joints the IK solver shall not move while looking for solutions. \\
		\hline
		bool avoid\_collisions & If true, the BioIK solver tries to find solutions that to not collide with the environment (and the robot itself). \\
		\hline
		RobotState robot\_state & The current state of the robot. Contains a JointState message containing the current joint angles of all joints. These values are taken as a starting point while searching for solutions. \\
		\hline
		PositionGoal[] position\_goals & The positions of multiple end-effectors that shall be reached with the IK solution. \\
		\hline
		PoseGoal[] pose\_goals & The poses of multiple end-effectors that shall be reached with the IK solution. A pose goal is similar to a position goal, but extends it by a desired orientation. \\
		\hline
		OrientationGoal orientation\_goals & The orientations multiple end-effectors shall have within the IK solution. \\
		\hline
	\end{tabularx}
\end{table}

\begin{table}
	\caption{Important contents of the IKResponse message type\label{tab:msg:ikresponse}}
	\begin{tabularx}{\linewidth}{|l|X|}
		\hline
		\textbf{field} & \textbf{Description} \\
		\hline
		MoveItErrorCodes error\_code & An error code stating if a solution was found or not. 0 indicates success, whereas all other values indicate that no solution was found. \\
		\hline
		RobotState solution & If error\_code is 0, this field contains the found solution encoded in a JointState field with an angle for every joint. \\
		\hline
	\end{tabularx}
\end{table}

\section{Grasp Synergies}

In their work, \citeauthor{Bernardino2013} describe a way to record hand postures using a data glove and the Shadow C5 robotic hand. The result of their research are data recorded for 8 different grasp postures of the human hand \cite{Bernardino2013}. Using \textit{Principal Component Analysis (PCA)} the datasets for each posture were parametrized. This process resulted in a matrix $S = (s_1 \dots s_M)$ for each grasp posture with $s_m$ being the eigenvectors of the parametrized grasp postures. For each posture $s_0$ is the mean value of all recorded posture data sets, defining the rest position of the hand within this posture. To get joint angles from these synergy matrices, they have to be multiplied by a vector $\alpha = (\alpha_1 \dots \alpha_M)$ containing the so-called amplitudes for each parameter. For a given synergy matrix and amplitude vector $\alpha \in \mathbb{R}^N$, the joint angles $\theta$ are described as
\begin{equation*}
\theta = s_0 + S\alpha
\end{equation*}

$S$ is sorted in a way that changes to $\alpha_1, \alpha_2$ and $\alpha_3$ already cover approx. 80-90\% of the variance in the grasp postures recorded, which is why we will only look onto these three amplitudes when implementing the approach to grasping objects in this thesis.

$s_0$ and $S$ are provided by the above research, $\theta_n \in \theta$ is given in degrees, $\alpha_n \in \alpha$ are $-50 \leq \alpha_n \leq 50$. A good way has to be found to map touch gestures in a way that the three most significant amplitudes can be intuitively controlled. 

\subsection{Touch Gestures}

To find solutions to map touch gestures to synergy amplitudes, we first generalize the understanding of touch gestures.

\begin{defn}
Let $p = (p_x, p_y) \in \mathbb{R}^2$ be a pointer on a touch screen, i.e. the coordinates of a registered finger the user has laid onto the touch surface. Then a set of pointers $G = \{p_1\dots p_n\}$ with $|G| \geq 1$ is called a \textbf{gesture}.
\end{defn}

Although pointer positions on a touch screen are usually given in integer numbers, pointers are defined in real space to make the following definitions possible. After having defined gestures, we have a look at different properties of them. Firstly, the two most basic properties of a gesture are defined, being the \textbf{position} and the \textbf{size}.

\begin{defn}
	Let $G$ be a gesture.
	
\begin{itemize}
	\item The \textbf{position} $c(G)$ of G is defined as
	\begin{equation}
	c(G) = \frac{1}{|G|}\sum_{i=1}^{|G|}p_i, p_i \in G
	\end{equation}
	
	\item The \textbf{size} $s(G)$ of G is
	\begin{equation}
	s(G) = \frac{2}{|G|}\sum_{i=1}^{|G|} d(c(G), p_i)
	\end{equation}
	with $p_i \in G$ and $d(x, y) = \sqrt{(y_1 - x_1)^2 + (y_2 - x_2)^2}$ for $x, y \in \mathbb{R}^2$ the euclidian distance between two pointers.
\end{itemize}
\end{defn}

In other words: The position of a gesture is the \textit{center of mass} of all pointers of a gesture. The size is the doubled mean distance of all pointers to the position of a gesture. Accordingly, the diameter of a circle having the radius of the mean distance of the pointers to the position of a gesture is the size. Before defining the last property of a gesture, the \textbf{orientation} we first have to define what the \textbf{thumb pointer} of a gesture is.

\begin{defn}
	The \textbf{thumb pointer} $th(G)$ of a gesture $G$ is defined as
	\begin{equation}
	th(G) = \left\{
	\begin{array}{ll}
	p_1 & |G| = 1 \\
	p_n \text{ with } p_{ny} = max\{p_{iy} : p_i \in G\} & |G| = 2 \\
	p_n \text{ with } d(c(G), p_n) = max\{ d(c(G), p_i) : p_i \in G \}& |G| > 2
	\end{array}
	\right.
	\end{equation}
	
The thumb pointer shall be evaluated and memorized whenever $|G|$ changes, i.e. a pointer is added or removed from a gesture.
\end{defn}

Note that the $y$ coordinate is rising to the bottom, as it is usual on digital screens. Having this in mind the thumb pointer is the lowest pointer in a 2-pointer gesture or the one furthest away from the position of a gesture with 3 or more pointers. The last part of the definition is important, as the lowest pointer does not necessarily remain the lowest when the gesture is rotated on the screen, so to have a consistent definition of the orientation, the thumb pointer may only be evaluated when pointers are added or removed from a gesture.

\begin{defn}
	Let $G$ be a gesture, $b_y = (0, -1) \in \mathbb{R}^2$. The orientation $o(G)$ of $G$ is defined as
	
\begin{equation}
o(G) = acos\left(\frac{(c(G) - th(G)) \cdot b_y}{|c(G) - th(G)| \cdot |b_y|}\right)
\end{equation}	
	
\end{defn}

In other words the orientation of a gesture is the angle between the vector from the thumb pointer to the position of a gesture and $b_y$, which is pointing upwards in screen coordinates as, again $y$ is growing downwards.

\subsection{Absolute Approach}
\label{sec:synergies:absolute}

\subsection{Relative Approach}

\subsection{Arm Control}

\section{Direct Fingertip Mapping}

\section{Software Architecture}